% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{edm_template}

\usepackage{bm}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{tikz}
%\usepackage{xcolor}

<<init,include=FALSE>>=
library(knitr)
opts_chunk$set(
echo=FALSE,results='asis',warning = FALSE,message = FALSE,cache=TRUE,error = FALSE
)
pn <- function(...) prettyNum(...,big.mark=',')
# library(xtable)
# library(lme4)
# library(tidyverse)
# library(arm)
# library(RItools)
# library(optmatch)
# library(estimatr)
# library(lmerTest)
# select <- dplyr::select

# `%nin%` <- Hmisc::`%nin%`

@

\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}


<<loadResults,include=FALSE,cache=FALSE>>=
load('artifacts/dataAnalysis.RData')
@


\begin{document}

\title{The effect of teachers reassigning students to new Cognitive Tutor sections}
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Adam C Sales\\
       \affaddr{University of Texas College of Education}\\
       \affaddr{Austin, TX, USA}\\
       \email{asales@utexas.edu}
% 2nd. author
\alignauthor
John F Pane\\
       \affaddr{RAND Corporation}\\
       \affaddr{Pittsburgh, PA, USA}\\
       \email{jpane@rand.org}
}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
The design of the Cognitive Tutor Algebra I (CTA1) intelligent
tutoring system assumes that students work through sections of
material following a pre-specified order, and only move on from one
section to the next after mastering the first section’s
skills. However, the software gives teachers the ability to override
that structure, by reassigning students to different sections of the
curriculum. Which students get reassigned? Does reassignment hurt
student learning? Does it help? This paper used data from the
treatment arm of a large effectiveness study of the CTA1 curriculum to
estimate the effects of reassignment on students’ scores on an Algebra
I posttest. Since reassignment is not randomized, we used a multilevel
propensity score matching design, along with assessments of
sensitivity to bias from unmeasured confounding, to estimate the
effects of reassignment. We found that reassignment reduces posttest
scores by roughly 0.2 standard deviations—about the same as the overall CTA1 treatment effect—that unmeasured confounding is unlikely to completely explain this observed effect, but that the effect of reassignment may vary widely between classrooms.
\end{abstract}

%% A category with the (minimum) three required fields
%\category{H.4}{Information Systems Applications}{Miscellaneous}
%%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]
%
%\terms{Theory}


\section{Introduction}
Two closely related pillars of intelligent tutoring systems are
sequencing and mastery learning.
It has long been obvious that the sequence in which students learn
different topics is an important component of a curriculum, due to
prerequisites---for instance, students must master arithmetic in order
to learn how to solve algebraic equations.
A related example is scaffolding, in which
learners gradually achieve independence over a sequence of problems;
scaffolding ``consists essentially of the adult 'controling' those
elements of the task that are initially beyond the learner's capacity,
thus permitting him to concentrate upon and complete only those
elements that are within his range of competence''
\cite{wood1976role}.
However, measuring the effects of sequencing \cite{ritter2007order}
\cite{doroudi2016sequence} and determining prerequisites or optimal
sequences \cite{scheines2014discovering} \cite{vuong2011method} \cite{koedinger2002toward}
remains an active area of research.

By ``mastery learning,'' we mean the idea that students should
``progress through  topics  as  they  master  them,''
\cite{ritter2013predicting} as opposed to at a fixed pace.
This typically results in students within the same classroom working
on different parts of a curriculum at the same time.

The Cognitive Tutor Algebra I (CTA1) system
\cite{corbett2001cognitive} includes both features.
A particular Algebra I curriculum is programmed into the software, so
that students, if left alone, will encounter topics in a specific,
intentional sequence.
Mastery learning governs how they progress from one section to the
next: an underlying knowledge tracing model estimates the probability
students have mastered a set of pre-defined skills as they work
through problems that incorporate those skills.
Students ideally progress from one section to the next only after
demonstrating mastery on the previous section's skills.

Mastery learning does not always proceed this way in the CTA1
software.
After a student has worked a certain, pre-specified number of problems
in a particular section, he or she is automatically promoted to the
next section, even if he or she has not mastered its skills
\cite{aoas}.
Teachers can also reassign students working on one section to work on
an entirely different section.
If a teacher reassigns a student to a section other than the next one
in the sequence, reassignment violates the intended sequencing as well
as mastery learning.

There are a number of reasons teachers may want to meddle in the
automatic progress of students through a curriculum \cite{descriptivePaper}.
If a teacher observes an advanced student spending time on basic
skills, the teacher may move the student to more advanced sections.
If certain skills will be on a standardized test, and a teacher wants
all students to have had exposure to those skills before the test, the
teacher may reassign all of his or her students to work on a section
covering those skills.
If a teacher notices a student falling behind his or her peers in the
classroom, the teacher may choose to reassign the student to the
section that the rest of the class is working on, even if the student
has not demonstrated mastery on prerequisite skills (at least, within
the tutor).
If a teacher disagrees with the method a certain CTA1 section employs
in teaching an Algebra topic, the teacher may reassign students out of
that section, perhaps to the next unit or section in the curriculum.

It is unclear whether reassignment benefits students.
On the one hand, it violates the design principles of the software.
On the other hand, it allows teachers flexibility to teach the
material as they see fit, and use the tutor to meet the particular
needs of their classrooms.

This paper uses data from a large randomized trial of the CTA1
curriculum to estimate the effect of reassignment.
Unfortunately for our purposes, reassignment itself was not
randomized---the study was designed to estimate CTA1's effectiveness,
so access to the tutor was randomized instead.
Still, log data from study participants includes data on how often
each student was reassigned from one section to another, and posttests
measure their algebra skills at the end of the study.
For those reasons, this data provides a rare opportunity to measure
the effect of reassignment, and, by extension, the (joint)  importance
of topic sequencing and mastery learning.

The following section gives background on the effectiveness trial
and describe the data we will use for the study.
Section \ref{sec:psm} describes propensity score matching, the
method we employ. Section \ref{sec:propensityScores} describes the
propensity score models, which in turn describe characteristics of
students who are reassigned. Section \ref{sec:match} describes the
matching algorithm and covariate balance. Section \ref{sec:effects}
gives our main results on the effects of reassignment, including
sensitivity analysis to confounding from unmeasured covariates and
between-classroom effect heterogeneity. Section \ref{sec:discussion} concludes.

\section{Data: the RAND CTA1 Effectiveness Study}\label{sec:data}
In the years 2007--2010, the RAND Corporation conducted a randomized
study to test the effectiveness of the CTA1 curriculum relative to
business as usual.
The study tested CTA1 under authentic, natural conditions---that is,
oversight and support of CTA1's use was the same as it would have been
outside of an RCT.
The study population consisted of over 25,000 students in 73 high schools and 74 middle
schools located in 52 diverse school districts in seven states.
Students in Algebra I classrooms in participating schools took an algebra I
pretest and a posttest, both from the CTB/McGraw-Hill Acuity series.
The pretest was the Algebra Readiness Exam, a 40-item multiple-choice
exam testing students' algebra I prerequisite skills.
The posttest was the Algebra Proficiency Exam, a 32-item
multiple-choice exam testing algebra I skills including solving
equations for an unknown, graphing linear and quadratic functions,
calculating complex algebraic expressions and other skills.
Data from both exams were scored with a three-parameter item
response theory (IRT) model.

Results \cite{pane2014effectiveness} were reported separately for
middle and high schools, in the first and second years of
implementation.
In the first year, estimated effects were close to zero in middle
schools and slightly negative in high schools, with confidence
intervals including negative, null, and positive effects in both
cases.
In the second year, estimated effects were positive---roughly one fifth of a
standard deviation---in both middle and high schools, and were
statistically significant in high schools.
In the high school sample, the difference between the effects in the
first and second years was statistically significant as well.

As part of the study, RAND collected basic demographic data from
students, including gender, race/ethnicity, prior standardized test
scores, and special education, free or reduced-price lunch, and
English language learner status.

Carnegie Learning collected computer log data from most users in the
treatment arm of the study.
At the problem level, this dataset records which problems students
attempted, along with timestamps and the numbers of hints and errors
for each attempted problem.
The dataset also contains data on which sections of CTA1 students
attempted, and the result: whether the student mastered the section,
was promoted automatically without mastery, was reassigned by the
teacher to a new section, or stopped using the tutor altogether midway
through the section.

The current study analyzes data from the high school treatment group only,
assessing the effect of teachers reassigning students from one CTA1
section to another.
Since students in the control arm of the study did not have access to
the tutor, section reassignment is not relevant for them.
We focus on high school, as opposed to middle school, since the
characteristics of Algebra I students tend to differ between the two
levels: 8th-grade students only take Algebra I if they are
sufficiently advanced, whereas most 9th grade students (who have not
taken it already) take Algebra I regardless.
Thus, the high school sample was not only larger but also more broadly
representative than middle school sample.

<<include=FALSE,cache=FALSE>>=
attach(dataInfo)
@

Unfortunately, log data was not available for every student in the
treatment arm of the study, primarily for two reasons: some students
in CTA1 schools nevertheless did not use the tutor, and some students
used the tutor but their log data was irretrievable or could not be
reliably linked to posttest scores and covariates.
This study omitted schools in which data was missing for over 20\% of
students in either year, leaving \Sexpr{pn(nschool)} schools.
Among the students at these schools, we omitted 164 had no log data, and 242
either only worked---but did not complete---one section or had no
section completion data for some other reason.
A total of \Sexpr{pn(nstud)} students in \Sexpr{pn(nclass)} classrooms
remained in the analysis sample, roughly \Sexpr{percentKept}\% of the
full treatment group.

\input{tables/ncp}

Table \ref{tab:ncp} shows the number of included students in each year of the
experiment who were reassigned zero, one, two, three, or four or more times.
Since the sample size decreases quickly with the number of
reassignments, and for the sake of simplicity, we chose to dichotomize
reassignment, estimating the effect of being reassigned at least once
versus never.



\section{Statistical Approach}\label{sec:psm}
For subjects $i=1,\dots,N$ in the treatment arm of the CTA1 trial, let
$Y_i$ denote subject $i$'s posttest score, and let $Z_i\in \{0,1\}$
indicate whether $i$ was ever reassigned.
Following \cite{neyman} and \cite{rubin}, let $y_i^0$ and $y^1_i$ denote $i$'s
posttest score were $Z_i=0$ or $1$---i.e., had $i$ not been
reassigned, or had $i$ been reassigned, perhaps counterfactually---and
let $\tau_i=y_i^1-y_i^0$ be the effect of reassignment on $i$'s
posttest score.
Since $y_i^1$ and $y_i^0$ are never simultaneously observed, $\tau_i$
is unidentified; however, weighted average treatment effects of the form
$\tau^w=\sum_i w_i \tau_i$, with $w_i\ge 0$ and $\sum_iw_i=1$ may be
identified under the right causal assumptions.
For instance, had $Z$ been randomized, the average treatment effect,
$\tau^w$ with $w_i=1/N$, could be estimated without bias by the
difference in the mean of $Y$ between subjects with $Z=1$ and with
$Z=0$.
Of course, reassignment $Z$ was not random, so identifying average
treatment effects requires some combination of control for observed
covariates and assumptions about unobserved covariates.

Let $\bm{x}_i$ denote a vector of covariates for subject $i$.
These include pretest scores, special education, gifted, and English
language learner (ELL) status,
race/ethnicity (white, black, Latinx\footnote{For the sake of
  parsimony, these categories were
  collapsed from a larger set in the original dataset, so that
  \Sexpr{nInd} American Indian/Alaskan Native students were
  categorized as Latinx, \Sexpr{nAsn} Asian/Pacific Islander students
  and \Sexpr{nRaceNA} students with missing data were categorized as white, and \Sexpr{nOth} Other/Multiracial
  students were categorized as black.}),
received free or reduced-price lunch (FRL).
Let $Class_i$ be $i$'s classroom; since reassignment occurred within
classrooms, $Class$ is a covariate as well.
If reassignment were randomly assigned, the (theoretical) distribution
of $\bm{x}$ and $Class$
would be equal between reassigned and not-reassigned
students---$\bm{x}$ and $Class$ would be balanced.
Our strategy will be to construct a randomization scheme in which
$\bm{x}$, and, to the extent possible, $Class$ are balanced, and
conduct inference under that randomization scheme.

Specifically, we use propensity score matching \cite{rosenbaum2002observational} \cite{rebar}.
The propensity score for subject $i$, $e_i(\bm{x}_i,Class_i)=Pr(Z_i=1|\bm{x}_i,Class_i)$ is the probability of $i$ being reassigned conditional on covariates $\bm{x}$ and classroom.
\cite{rosenbaum1983central} showed that under two conditions, described below, estimates of the average treatment effect conditional on $e(\bm{x},Class)$ are unbiased.
To estimate effects, we first estimate propensity scores (Section \ref{sec:propensityScores}), then identify groups of reassigned and not-reassigned students with similar estimated propensity scores---a ``match''---and verify that covariates are sufficiently balanced within the matched sample (Section \ref{sec:match}), and, finally, estimate effects within the matched sample \ref{sec:effects}.

The first condition for propensity score matching is that there is some randomness in the treatment assignment:
\begin{equation}\label{eq:commonSupport}
0<e_i(\bm{x}_i,Class_i)<1 \text{ for all }i.
\end{equation}
When \eqref{eq:commonSupport} fails for a subset of the analysis sample, common practice is to drop that subset and estimate average effects for the remainder of the analysis sample, i.e. the subset for which \eqref{eq:commonSupport} holds; this subset is referred to as the ``region of common support" \cite{caliendo2008some} \cite{shadish2010primer}.
In this study, including $Class$ among the covariates leads to violations of \eqref{eq:commonSupport}.
Of the \Sexpr{nclassTot}
classrooms over the two years of the study, \Sexpr{nclass0} contained no
reassigned students, and in \Sexpr{nclass1} classrooms every student
was reassigned at least once.
In this subset of the data, including \Sexpr{round(pstud10*100)}\% of
students, $Pr(Z=1|Class)=0$ or 1.
Our solution is to drop classrooms in which no one or everyone was
reassigned, and only estimate effects for students in classrooms with
some reassignment variance, a student-level analysis.

We attempted a parallel classroom-level analysis, in which we matched
classrooms in which all students were reassigned to classrooms in
which no one was.
However, we were unable to construct a match with adequate covariate
balance (there were few no-reassigned classrooms with similar mean
pretest scores to the all-reassigned classrooms that were of similar
sizes).
For that reason, we dropped the classroom-level analysis.
% In the second part, we conduct a classroom-level analysis.
% For for classrooms $j=1,...,J$, let $\bar{Z}_j=1$ if everyone in classroom $j$ was reassigned and $\bar{Z}=0$ if no one was reassigned.
% Then we estimate propensity scores $e(\bm{\bar{x}}_j)=Pr(\bar{Z}=1|\bm{\bar{Z}})$, where $\bm{\bar{x}}$ is a vector of covariates aggregated to the classroom level, and match $\bar{Z}=1$ classrooms to $\bar{Z}=0$ classrooms with similar propensity scores.
% Finally, we estimate effects for these classrooms by modeling $Z$ as assigned at the classroom level, as in a group randomized trial.
% We will present the student-level and classroom-level effect estimates separately as well as in a combined model.

The second condition for propensity score matching is that there are no unmeasured confounders:
\begin{equation}\label{eq:ignore1}
(y^1,y^0)\independent Z|\bm{x},Class
\end{equation}

Assumption \eqref{eq:ignore1} is well known as the Achilles heel of causal inference outside of RCTs.
\eqref{eq:ignore1} is untestable; its believability depends on what is understood about the process that underlies treatment assignment $Z$, and what covariates are available for control.
In our case, reassignment is poorly understood, and appears highly idiosyncratic \cite{descriptivePaper}.
Fortunately, our study includes a pretest measure, and observational studies controlling for pretest scores tend to perform well, and replicate experimental estimates \cite{cook2008three} \cite{cook2009bias}.
Section \ref{sec:sensitivity} discusses a sensitivity analysis that
relaxes \ref{eq:ignore1} and assumes reasonable levels of unmeasured
confounding.

Our attitude towards propensity score matching is agnostic.
If the propensity score models in the following section were
approximately correct, and yielded good estimates of the true
propensity scores, then the theory underlying propensity score
adjustment holds.
If not, the process of propensity score matching may still result in a
set of matched reassigned and not reassigned students that, on
average, resemble each other on all measured covariates.
In other words, the (mis)estimated propensity scores $\hat{e}$ may still be
approximate ``balancing'' scores, satisfying
\begin{equation}\label{eq:balancing}
\bm{x}\independent Z|\hat{e}.
\end{equation}
Causal inference based on comparisons within these matched sets will
still be plausible; indeed, \cite{rosenbaum1983central} showed that in order to estimate
average treatment effects, it is sufficient to condition on a
balancing score, rather than the propensity score itself.

Following that logic, we choose propensity score models, and matching
schemes based on the fitted models, in order to satisfy
\eqref{eq:balancing}.
Since posttest scores play no role in propensity score estimation and
matching, the process may be iterative without affecting the
objectivity of the final causal estimate.
That is, we may try a series of candidate propensity score models and
matches, and choose the one that results in the best covariate
balance.
Only then do posttests enter the picture, so that we may estimate
effects.

All data analysis was done in \texttt{R} \cite{rcite} using the
\texttt{tidyverse} suite of packages \cite{tidyverse} for data
manipulation, plotting, and other tasks.
This document was produced dynamically with \texttt{knitr}
\cite{knitr}.
Source code is available at www.github.com/[Redacted].

\section{Propensity Scores: Who Gets Reassigned?}\label{sec:propensityScores}
We use multilevel logistic regression \cite{gelmanHill} to estimate student level propensity scores.
The multilevel regression accounts for the nesting of students within classrooms, classrooms within teachers, and teachers within schools.
In constructing the model, we give special consideration to the role of pretest scores, a proxy for student mathematical ability at the beginning of the school year, in predicting reassignment.
First, we decompose pretest scores into student- and classroom-level components.
If $w_i$ is student $i$'s pretest score, let $w_i=\bar{w}_{j[i]}+\tilde{w}_i$, where $\bar{w}_{j[i]}$ is the average pretest score in $i$'s classroom $j[i]$, and $\tilde{w}_i$ is the difference between $i$'s pretest score and the classroom mean.
This decomposition was motivated by the possibility that reassignment patterns may differ between high- and low-achieving classrooms, and that a teacher's decision to reassign a student depends on the student's ability relative to the classroom than his or her absolute ability.
Second, we modeled the effect of $\tilde{w}$ on $Z$ as linear in the logit scale, but allowed the slope to vary by classroom.
This was motivated by the possibility that some teachers use reassignment to help struggling students catch up to their peers, so lower $\tilde{w}$ would predict $Z$, and other teachers use it to help high-achievers skip sections related to basic skills, so higher $\tilde{w}$ would predict $Z$.
We also considered models incorporating non-linear effects of
$\tilde{w}$, via natural splines \cite{hastie2017generalized} but
found no evidence that the non-linearity improved the model fit.
We fit the model using the \texttt{lme4} package in \texttt{R}
\cite{lme4}.

All in all, the propensity score model was:
\begin{equation}\label{eq:propmod1}
\begin{split}
log&it\left\{Pr(Z_i=1|\bm{x}_i,Class_i=j)\right\}=\\
& \beta_{0state[i]}+\beta_{1} \tilde{w}_i+\beta_2 \bar{w}_{j[i]}+\\
&\beta_3 Black_i+\beta_4 Latinx_i+\beta_5 Male_i+\\
&\beta_6 Freshman_i+\beta_7 SpEd_i+\beta_8 gifted_i+\\
&\beta_9 ESL_i+\beta_{10} FRL_i+\beta_{11}
FRLmis_i+\beta_{12}year_i+\\
&\gamma_{j[i]}\tilde{w}_i+\epsilon^{Cls}_{j[i]}+\epsilon^{Teach}_{k[i]}+\epsilon^{Schl}_{l[i]}
\end{split}
\end{equation}
where $logit(x)=log(x/(1-x))$ is the logit function,
$\beta_{0state[i]}$ is a (fixed) intercept for each state in the
sample, $FRLmis_i$ is an indicator for missing data in $FRL$ (which
was mode-imputed), and $year_i=1,2$ is the study year for subject $i$.

Finally, $\gamma_{j[i]}$, $\epsilon^{Cls}_{j[i]}$,
$\epsilon^{Teach}_{k[i]}$, and $\epsilon^{Schl}_{l[i]}$ are random
effects.
The subscripts $j$, $k$ and $l$ refer to classroom, teacher, and
school, respectively; the $[i]$ refers to student, so that $j[i]$ is
$i$'s classroom, $k[i]$ is $i$'s teacher, and $l[i]$ is $i$'s school.

$\gamma_{j[i]}$ is a random slope for $\tilde{w}_i$, varying at the
classroom level.
This is essentially an interaction term, allowing the slope for
(classroom centered) pretest scores to vary from one classroom to the
next.
However, unlike standard regression interactions, random slopes are
modeled as being drawn from a normal distribution, with a standard
deviation estimated from the data.
This is a form of regularization, shrinking the classroom-level slopes
towards a common value, and allowing stable estimation even with very
few observations from each classroom \cite{gelmanHill}
\cite{partialPoolingEDM}.
The set of random slopes $\gamma_j$ has a mean of zero---the
average slope across classrooms is the fixed intercept $\beta_1$.
Therefore, the slope for pretest in classroom $j$ is
$\beta_1+\gamma_j$.

$\epsilon^{Cls}_{j}$, $\epsilon^{Teach}_{k}$, and $\epsilon^{Schl}_{l}$
  are random intercepts for classroom, teacher, and school.
These were also modeled as normal with a mean of zero and a standard
deviation estimated from the data.
Including them in the regression accounts the fact
that two students in the same classroom or with the same teacher or in
the same school may be more likely to have the same $Z$---either both
be reassigned or neither---than two students in different classrooms,
with different teachers, or in different schools.


\begin{figure}
<<coefplot1,out.width="95%">>=

pdat <- psmodSumm$coef%>%
  as.data.frame()%>%
  rownames_to_column('Covariate')%>%
  filter(Covariate%in%c('pretestC','mpretest','raceBlack','raceHispanic','sexM','grade9','spec_speced', 'spec_gifted','spec_esl','frl','frlMISTRUE','year'))%>%
  mutate(
    Covariate= map_chr(Covariate,
      ~switch(.,
        mpretest='wbar',#"$\\bar{w}$",
        pretestC='wtilde',#"$\\tilde{w}$",
        raceWhite='White',
        raceBlack='Black',
        raceHispanic='Latinx',
        sexF='F',
        sexM='Male',
        "grade10+"="10+",
        grade9="Freshman",
        spec_speced="Special Ed.",
        spec_gifted="Gifted",
        spec_esl="ESL",
        frl="FRL",
        frlMISTRUE="FRL Missing",
        year="Year")
    ),
    Covariate=factor(Covariate,levels=rev(unique(Covariate))),
    ymax=Estimate+2*`Std. Error`,
    ymin=Estimate-2*`Std. Error`
  )

ggplot(pdat,aes(Covariate,Estimate,ymin=ymin,ymax=ymax))+
      geom_point(size=3)+
          xlab(NULL)+ylab(NULL)+
  geom_errorbar(width=0,size=2)+geom_hline(yintercept=0)+
  scale_x_discrete(labels=c(levels(pdat$Covariate)[1:(nlevels(pdat$Covariate)-2)],bquote(bar(w)),bquote(tilde(w))))+
  theme(text = element_text(size=25))+
  coord_flip()

@
%\input{plots/coefplot1.tex}
\caption{Estimated coefficients and 95\% confidence intervals for
  student and class-level covariates from model \eqref{eq:propmod1}.}
\label{fig:propmod1}
\end{figure}

Figure \ref{fig:propmod1} gives estimated coefficients and 95\%
confidence intervals for the propensity score model
\eqref{eq:propmod1}.
Reassignment was much more prevalent in the second year of
implementation than in the first, and classrooms with low average
pretest scores reassigned students more often---though the magnitude
of this trend is hard to determine, ranging from moderate to very
large (the coefficients for $\bar{w}$ and $\tilde{w}$ were scaled by the
standard deviations of these variables in the data).
Latinx students were reassigned more often than their White
classmates.

Students with lower pretest scores were reassigned more frequently
than their classmates with higher scores.
However, this may vary by classroom.
On average, classroom-specific $\beta_{1j}$ was approximately
\Sexpr{round(psmodSumm[['coef']]['pretestC',1],2)} standard
deviations, but the 95\% confidence interval for the mean includes
slightly positive values as well.
The standard deviation of $\beta_{1j}$, varying by classroom, was
estimated as \Sexpr{round(sdSlope,2)}, suggesting that in some
classrooms the slope on $\tilde{w}$ was moderately positive, and in
others it was negative.
However, the model was not able to estimate the variance of
$\beta_{1j}$ precisely; the p-value testing the null hypothesis of
zero variance was \Sexpr{round(sdSlopep,2)}.\footnote{This hypothesis
  was tested with a likelihood ratio $\chi^2$ test comparing
  \eqref{eq:propmod1} to a model in which $\beta_1$ did not vary by
  classroom.}
When model \eqref{eq:propmod1} was modified so that $\beta_1$ was not
allowed to vary by classroom, it was estimated as \Sexpr{beta1.0}.

% \subsection{Classroom-Level}
% Using data from classrooms in which either all students or no students
% were reassigned, we estimated classroom-level propensity scores
% $Pr(\bar{Z}=1|\bm{\bar{x}})$.
% To do so, we used a single-level logistic regression model:
% \begin{equation}\label{eq:propmod2}
% \begin{split}
% Pr(&\bar{Z}_j=1|\bm{\bar{x}}_j)=\\
% &logit^{-1}( \gamma_{0state[j]}+\gamma_1 \bar{w}_{j}+\\
% &\gamma_2 \%Freshman_j+\gamma_{3} \%FRL_j+\\
% &\gamma_{4}year_j+\gamma_5Nstud_j)
% \end{split}
% \end{equation}
% where $\%Freshman_j$ and $\% FRL_j$ denote the percentages of
% classroom $j$ that are 9th graders or receive free or reduced-price
% lunches, and $Nstud_j$ is the number of students in classroom $j$ in
% our dataset.
% Model \eqref{eq:propmod2} was simpler than the student-level version,
% \eqref{eq:propmod1}, both because it used classroom-aggregated data,
% and because the sample size was smaller ($n=\Sexpr{nclass1+nclass0}$
% classrooms versus $n=nStud1$ students), requiring greater parsimony.
% As described in Section \ref{sec:psm}, model \eqref{eq:propmod2} was
% selected in order to acheive covariate balance \eqref{eq:balance}.

% \begin{figure}\label{fig:coefplot2}
% <<coefplot2,fig.width=3,fig.height=2>>=

% pdat2 <- psmod2coef%>%
%   as.data.frame()%>%
%   rownames_to_column('Covariate')%>%
%   filter(Covariate%in%c('nstud','mpretest','raceBlack','raceHispanic','sexM','grade9','spec_speced', 'spec_gifted','spec_esl','frl','frlMISTRUE','year'))%>%
%   mutate(
%     Covariate= map_chr(Covariate,
%       ~switch(.,
%         mpretest='wbar',#"$\\bar{w}$",
%         pretestC='wtilde',#"$\\tilde{w}$",
%         raceWhite='White',
%         raceBlack='Black',
%         raceHispanic='Latinx',
%         sexF='F',
%         sexM='Male',
%         "grade10+"="10+",
%         grade9="%Freshman",
%         spec_speced="Special Ed.",
%         spec_gifted="Gifted",
%         spec_esl="ESL",
%         frl="%FRL",
%         frlMISTRUE="FRL Missing",
%         year="Year",
%         nstud="Nstud")
%     ),
%     Covariate=factor(Covariate,levels=rev(unique(Covariate))),
%     ymax=Estimate+2*`Std. Error`,
%     ymin=Estimate-2*`Std. Error`
%   )

% ggplot(pdat2,aes(Covariate,Estimate,ymin=ymin,ymax=ymax))+
%       geom_point(size=3)+
%           xlab(NULL)+ylab(NULL)+
%   geom_errorbar(width=0,size=2)+geom_hline(yintercept=0)+
%   scale_x_discrete(labels=c(levels(pdat2$Covariate)[1:(nlevels(pdat2$Covariate)-1)],bquote(bar(w))))+
%   theme(text = element_text(size=10))+
%   coord_flip()

% @
% \caption{Estimated coefficients and 95\% confidence intervals from
%   model \eqref{eq:propmod2}.}
% \label{fig:propmodel2}
% \end{figure}

% Figure \ref{fig:propmodel2} shows estimates and 95\% confidence
% intervals for covariate coefficients from \eqref{eq:propmodel2}.
% Similar to the results from the student-level analysis, reassignment
% is more prevalent in classrooms with lower average pretest scores and
% in the second year of implementation, though \eqref{eq:propmodel2}
% cannot rule out null, or very small opposite-signed effects in either case.
% Teachers in classrooms with a greater proportion of FRL students were less likely
% to use reassignment.


% \begin{figure}
% <<psSlopes,out.width="95%",dev='tikz',dev.args=list(pointsize=200)>>=
% ggplot(slopeDat)+geom_abline(aes(slope=b1,intercept=b0,group=id,color=what))+coord_cartesian(xlim=c(xmin,xmax),ylim=yrange,expand=FALSE)+labs(x="$\\tilde{w}$",y="Log Odds of Reassignment",color=NULL)+theme(legend.position='top')
% @
% \end{figure}



\section{Matching and Covariate Balance}\label{sec:match}
%\subsection{Student Level Match}
We construct a student-level match based on propensity scores on the
log-odds scale, i.e. $log(\hat{e}/(1-\hat{e})$.
Instead of a pair-matching design, which would necessitate discarding
non-reassigned students who would make good matched comparisons, we
use a restricted full match design \cite{hansen2004}.
In this design, the numbers of reassigned and not-reassigned students
in each matched set is allowed to vary, so that in some cases several
reassigned students may be matched with a single non-reassigned
student, and vice-versa.
We use the \texttt{R} package \texttt{optmatch} \cite{optmatch} to
choose the matched sets optimally.
The \texttt{fullmatch()} routine takes a matrix of discrepancies
(e.g. differences in propensity scores) between treatment and control
subjects, and arranges them into matched sets so that the sum of
absolute discrepancies between matched subjects is minimized.

As described at the end of Section \ref{sec:psm}, the post-test scores
played no role in this process.
Hence, we were able to iteratively match students, check covariate
balance, modify the propensity score model and/or the matching routine
if necessary, and repeat until adequate balance was achieved.
Here we present the final match; a record of attempts is available on
the first author's github site.

\begin{figure}
\includegraphics[width=0.95\linewidth]{plots/psPlot1.pdf}
\caption{Estimated propensity scores for reassigned and not-reassigned
  students. Scores for students who were excluded from the ultimate
  match are colored red.}
\label{fig:psplot1}
\end{figure}

The initial full match based on the log-odds propensity scores yielded
decent covariate balance. However, pretest scores were slightly
unbalanced, and since we consider pretest to be the most important
covariate, we decided to match on the Mahalanobis distances between
reassigned and not reassigned students combining propensity scores and
pretest scores.
Additionally, as displayed in Figure \ref{fig:psplot1}, the
distributions of propensity scores among reassigned and not-reassigned
students do not entirely overlap.
Although this is at least partially due to overfitting the propensity
score model \eqref{eq:propmod1}, matching students with highly
discrepant propensity scores may hinder the believability of the
result.
Hence, in our final match we imposed a caliper of 0.3 pooled standard
deviations of the Mahalanobis distances.
This prevented students with very different pretest scores or
propensity scores to be matched.
On the other hand, matches were unavailable for \Sexpr{round(propExcluded1*100)}\% of the
students in the sample (\Sexpr{round(propExcluded11*100)}\% of
reassigned students and \Sexpr{round(propExcluded10*100)}\% of
not-reassigned students).
Propensity scores for these students are colored red in Figure \ref{fig:psplot1}.
Our effect estimates pertain only to the remaining
\Sexpr{round((1-propExcluded1)*100)}\% of students---all in all,
\Sexpr{nIncluded1} students, \Sexpr{nIncluded11} reassigned and
\Sexpr{nIncluded10} not reassigned.

\begin{figure}
\includegraphics[width=0.95\linewidth]{plots/balance1-1.pdf}
\caption{Covariate balance (standardized differences) before and after
  matching, for student level data. Dotted lines indicate standardized
differences of 0.25 and 0.05, following the What Works Clearinghouse
standards.}
\label{fig:balance1}
\end{figure}

Covariate balance after matching was excellent.
Figure \ref{fig:balance1} and Table \ref{tab:balance1} give covariate
balance (standardized differences) before and after matching.
They were produced with the \texttt{RItools} package in \texttt{R}
\cite{ritools}.
Before matching, several covariates were unbalanced, especially race.
Table \ref{tab:balance1} shows stars reflecting p-values from
individual covariate balance tests; nearly all covariates were
unbalanced at the $\alpha=0.1$ level.
An omnibus balance test \cite{covBal} gives $p<0.001$.
Figure \ref{fig:balance1} shows, as benchmarks, standardized
differences of $\pm$ 0.25 and 0.5, corresponding to thresholds given
in the What Works Clearinghouse (WWC) handbook\footnote{In the context of a randomized experiment with attrition, covariate
imbalances with standardized differences greater than 0.25 invalidate
a study, whereas differences between 0.05 and 0.25 require statistical
adjustment and differences less than 0.05 are acceptable as is.}  \cite{wwc}.
Before matching, imbalances in race and FRL missingness exceeded 0.25,
and most other imbalances were greater than 0.05.

Matching improved nearly all of these imbalances.
Most importantly, pretest measures were nearly exactly balanced.
None of the individual covariate balance tests was significant at the
10\% level or had standardized differences greater than 0.25, and,
with the exception of race, and FRL missingness none of the covariates was imbalanced with
a standardized difference greater than 0.05.
The omnibus p-value testing overall balance was
\Sexpr{round(covsbal,2)}.

The match also balanced classroom indicators.
Before matching, the omnibus p-value testing balance of classroom
indicators was $<0.001$; after matching it was $0.99$.

\input{tables/balance1.tex}
% \subsection{Classroom Level Match}

% \begin{figure}\label{fig:covBal2}
% \includegraphics[width=\textwidth]{plots/balPlot2}
% \caption{Covariate balance (standardized differences) before and after
%   matching, for student level data. Dotted lines indicate standardized
% differences of 0.25 and 0.05, following the What Works Clearinghouse
% standards. Standardized differences account for the clustering of
% students within classrooms.}

% Unfortunately, due to the smaller sample size, the classroom-level
% match did not exhibit the same properties as the student-level match.
% We report the match and its covariate balance here, and the associated
% effect estimates in the following section, but these should be taken
% with a grain of salt.

% Unlike in the student-level analysis, a full classroom match based only on linear
% predictors (log odds) from model \eqref{eq:propmod2} performed better
% than the alternatives we considered.\footnote{These included a
%   multilevel match using the ??? package in \texttt{R} \cite{} which
%   explicitly accounts for the multilevel structure of the data.}

% We assess covariate balance using student-level data, accounting for
% the clustering of students within classrooms using the methods
% described in \cite{covBal}.
% The resulting covariate balance is displayed in Figure
% \ref{fig:covBal2} and in Table \ref{tab:covBal2}.
% Matching improved balance considerably on Race and free or
% reduced-price lunch status, and the balance on class-average pretest
% after matching is nearly perfect.
% However, matching slightly reduced balance on a few covariates, and
% standardized differences for several covariates are outside the 0.05
% threshold, even after matching.
% The standardized difference on year was outside of the 0.25 threshold,
% even after matching.


\section{The Effect of Reassignment}\label{sec:effects}
\input{tables/effectsStud.tex}
Table \ref{tab:effectsStud} gives five estimates for the effect of
reassignment in classrooms where some students, but not all, were
reassigned at some point.
The first column gives the estimate itself, the second gives the
sample size $N$ for that estimate, the third, ``Std Error'' gives the
standard error, and the fourth, ``CI,'' gives a 95\% confidence
interval.
The last two columns contain sensitivity analyses, described in the
following section.
All the estimates used a regression routine from the \texttt{estimatr}
package in \texttt{R} \cite{estimatr}, with ``HC2''
heteroskedasticity-robust standard errors.

The first row, labeled ``Raw,'' is an an unadjusted estimate,
comparing all students in the sample who were reassigned to all
students who weren't.
There is little difference in their average posttest scores.

The next row, labeled ``Matched+Regression,'' gives the effect
estimate based on the match from Section \ref{sec:match}.
The lower sample size \Sexpr{mod1[['N']]} reflects the fact that some
students were excluded from the match; this estimate only pertains to
those who were included.
To estimate the effect, we regress posttests on $Z$ including a fixed
effects for each match.
Let $\hat{\tau}_m$ be the estimated effect in match $m$.
If $m$ is a pair---one reassigned student matched with one
non-reassigned student---then $\hat{tau}_m$ is the difference between
the two students' posttest scores.
If there are more than two students in the match, $\hat{\tau}_m$ is
difference in posttest means between reassigned and not-reassigned
students within matched-set $m$.
If treatment assignment is unconfounded within each match, $Z
\independent \{y_C,y_T\}|match$, then $\hat{\tau}_m$ is unbiased for
the average effect of $Z$ on posttest scores in match $m$.
Then the regression estimate is a weighted average of $\hat{\tau}_m$,
with weights $w_m\propto (1/n_{1m}+1/n_{0m})^{-1}$; this weighing
scheme minimizes the standard error under standard linear regression
assumptions (if the regressions assumptions do not hold, but $Z$ is
still unconfounded within the match, then the estimate is still
unbiased but the weights are sub-optimal).

The next row, labeled ``Match+Regression'' uses the same regression
model as the ``Matched'' estimator, but additionally controls for
pretest scores (with a natural spline with five degrees of freedom),
and indicators for special education status, missing free or
reduced-price lunch data, and race.
This strategy controls for differences in these covariates left over
after the match, accounting for the fact that the match was imperfect.

The ``Matched'' and ``Match+Regression'' estimates were almost
identical---effect sizes of \Sexpr{round(mod1$coef['everCPTRUE'],2)}
and \Sexpr{round(mod2$coef['everCPTRUE'],2)}, respectively, with 95\%
confidence intervals of
\Sexpr{with(mod1,printci(c(conf.low['everCPTRUE'],conf.high['everCPTRUE'])))}
an
\Sexpr{with(mod2,printci(c(conf.low['everCPTRUE'],conf.high['everCPTRUE'])))}.
These negative effect estimates suggest that reassignment hurts
student learning.
The effect size of a fifth of a standard deviation is roughly the same
as the overall average effect of CTA1 in high schools in the second
year of implementation, as estimated in \cite{pane2014effectiveness},
suggesting that reassignment may negate most of the positive effect of
using CTA1.

The next two rows of Table \ref{tab:effectsStud}, however, suggest that the effect of reassignment
may depend on context.
Each row uses the ``Match+Regression'' approach, but separately in
data from implementation years 1 and 2.
It appears that reassignment may have hurt students' posttest scores
more in the first than in the second year of implementation---in the
first year, we estimate an effect of
\Sexpr{round(mod3.1$coef['everCPTRUE'],2)} and in the second year we
estimate an effect of \Sexpr{round(mod3.2$coef['everCPTRUE'],2)}.
That said, the difference between the two effects is not itself
statistically significant---that is, it may be the result of
statistical noise.

The final row of Table \ref{tab:effectsStud}, labeled ``Within-Class,'' uses a different
confounder control strategy altogether.
This estimate matches students by classroom, as if
reassignment were randomized within classrooms.
To weaken that assumption, the ``Within-Class'' estimate incorporates
additional regression controls: a natural spline with five degrees of
freedom for pretest, and indicator variables for the remaining
covariates.
This strategy estimates a similar negative effect as the others,
\Sexpr{round(mod4[['coefficients']]['everCP'],2)}, with a 95\%
confidence interval of
\Sexpr{with(mod4,printci(c(conf.low['everCPTRUE'],conf.high['everCPTRUE'])))}.



\subsection{Unobserved Confounding}\label{sec:sensitivity}

The estimates in Table \ref{tab:effectsStud} all assumed
\eqref{eq:ignore1}, that there was no unobserved confounding.
This assumption is strong, untestable, and could undermine all of the
inference in Section \ref{sec:effects}.
For instance, the estimated negative effect may be due to baseline differences
in ability, beyond what is captured in pretest scores.

\citeN{hhh} suggest a method of estimating the sensitivity of a
regression to an omitted confounder based on benchmarking from
observed confounders.
Roughly speaking, the idea is to widen the confidence interval from an
ostensibly causal linear model to account for the possibility of a
hypothetical unmeasured confounder, $U$, that predicts reassignment and posttests to the same extent as one of the observed covariates.
These ``sensitivity intervals'' account for uncertainty from two sources: random error, and systematic error due to the omission of a confounder.

In order to confound the causal relationship between reassignment and posttests, a confounder would have to predict both.
Capturing these two requirements, the method of \citeN{hhh}
is based on two sensitivity parameters: first, $T_Z$ encodes the
extent to which $U$ predicts $Z$, after accounting for observed
covariates $\bm{x}$.
Formally, $T_Z$ is the t-statistic on the $U$ coefficient from an ordinary least squares
regression of $Z$ on $U$ and $\bm{X}$.
The second parameter is $\rho^2$, the squared partial correlation
between posttest scores and $U$, conditional on $\bm{x}$.
Of course, since $U$ is unobserved, neither $T_Z$ nor $\rho^2$ is
known; \citeN{hhh} suggest benchmarking them using observed
covariates.
That is, imagine each observed covariate, in turn, were unobserved,
and calculate its $T_Z$ and $\rho^2$ given the rest of the observed
covariates.

\sloppy
Table \ref{tab:effectsStud} includes two such sensitivity intervals.
The column labeled ``[Pretest]'' includes sensitivity intervals for an
unobserved confounder that predicts reassignment and posttests as well
as do pretest scores---typically the most important confounder.
That is, these intervals are 95\% confidence intervals that assume
the possible existence of an unmeasured covariate as important as
pretest.
It turns out, in the current analysis, that omitting state indicators
would cause more bias than omitting pretest scores; for that reason,
the column labeled ``[State]'' gives sensitivity intervals for an
unobserved confounder that predicts reassignment and posttest scores
as well as state indicators.
Both sets of sensitivity intervals are considerably wider than the
corresponding confidence intervals, including both large and small
negative effects.
Sensitivity intervals for the ``Matched'', ``Match+Regression,'' ``Year 1,'' and
``Within-Class'' estimates, whose confidence intervals excluded zero,
 excluded zero as well.
That is, confounding from an unobserved variable as important as
pretest or state may have led us to over-estimate the negative effect
of reassignment; it may have also led us to under-estimate the effect.
However, such confounding cannot explain the sign of the effect we
estimated---even assuming the existence of an unobserved confounder as
important as our most important covariates, the effect must be negative.

That said, an even stronger confounder, or more complex confounding
from several unobserved covariates, may explain the observed results.
Without a randomized trial, it is impossible to entirely rule out
unobserved confounding.

\subsection{Treatment Effect Heterogeneity}\label{sec:heterogeneity}

Previous research \cite{descriptivePaper} has found evidence for a
wide variety of uses for reassignment.
In some cases, teachers reassign students who are falling behind their
classmates, in other cases teachers reassign nearly the entire class
to work on a particular section of the tutor, and in other cases
teachers will simultaneously reassign all students working on a
particular section \emph{out} of that section.
Along similar lines, our (inconclusive) evidence for variance between
classrooms in the relationship between pretest scores and the
probability of a student being reassigned points towards varying uses
for reassignment.

If reassignment is used differently from classroom to classroom, it
stands to reason that it might have different effects in different
classrooms, as well.
To test that assumption, we fit a multilevel model with random effects
for reassignment, varying by classroom.
The model had the same fixed effects as model underlying the
``Match+Regression'' results described above, as well as random
intercepts for classroom and random slopes for reassignment, varying
by classroom.
Formally, the model is:
\begin{equation}\label{eq:heteroReg}
\begin{split}
y_i&=\beta_{0,m[i]}+\beta_1Z_i+\\
&\beta_2SpEd_i+\beta_3frlMIS_i+\beta_4Black_i+\beta_5Hisp_i+\\
&ns^5(pretest_i,\bm{\alpha})+\gamma_{j[i]}Z_i+\epsilon^{Cls}_{j[i]}+\epsilon^{Ind}_i
\end{split}
\end{equation}
where $\beta_{0,m[i]}$ is a fixed intercept for each match, $ns^5(pretest_i,\bm{\alpha})$ is a natural spline for pretest, with five degrees of freedom and coefficient vector $\alpha$, and $\gamma_{j[i]}$, $\epsilon^{Cls}_{j[i]}$, and $\epsilon^{Ind}_i$ are random effects, modeled as normal with mean zero and standard deviation estimated from the data.
Symbols $\alpha$, $\beta$, $\gamma$, and $\epsilon$ do not represent the same quantities as in equation \eqref{eq:propmod1}. 
$\gamma_{j[i]}$ is the random slope for reassignment, varying by classroom; the effect of reassignment in classroom $j$ is estimated as $\hat{\beta}_1+\hat{\gamma}_j$.
That is, $\beta_1$ represents the effect of reassignment, averaged over all classrooms, and $\gamma_j$ represents the difference between classroom $j$'s effect and the average. 
While precisely estimating the effect of reassignment in any particular
classroom is beyond the scope of our data, this model allows us to estimate the variance across those effects, as the variance of $\gamma_j$s.

\begin{figure}
\includegraphics[width=\linewidth]{plots/EffectByClassroom.pdf}
\caption{Classroom-specific effects of reassignment ($\hat{\beta}_1+\hat{\gamma}_j$) from model \eqref{eq:heteroReg}. Error bars represent standard errors.}
\label{fig:trtHet}
\end{figure}

The results are displayed in Figure \ref{fig:trtHet}.
The effect of reassignment in an average classroom is estimated as
similar to the effects in Table \ref{tab:effectsStud}.
This effect varies with a standard deviation of approximately
\Sexpr{round(sdEff,2)}.
To test for between-classroom variance, we compared the fit of the
multilevel model to an analogous model without random slopes, with a
likelihood ratio $\chi^2$ test; the p-value was
\Sexpr{signif(sdEffp,1)}.
This standard deviation is large enough to imply that the effect will
be positive in some classrooms---indeed, Figure \ref{fig:trtHet} shows
a number of classrooms with positive effects.
That said, the confidence intervals (based on estimates for the
conditional variance of random slopes, combined with the standard
error of the main effect of reassignment) are all rather wide and
nearly all contain zero.

Therefore, while the effect of reassignment was negative, on average, it may
have been positive in some classrooms.

\begin{figure*}
\centering
\includegraphics{plots/heterogeneityRegression.pdf}
\caption{The random effects $\gamma_j$ from model \eqref{eq:heteroReg} (with error bars for one standard error) as a function of classroom-level variance in pretest scores and the proportion of students in a classroom who were ever reassigned. OLS fits are added for interpretation.}
\label{fig:hetRegs}
\end{figure*}


This variation could be due to a number of factors, including
differences in the composition of classroomsand in when and how reassignment is used.
We considered two simple hypotheses about classroom-level predictors of heterogeneous treatment effects.
The first hypothesis was that variance in students' pretest scores within a classroom predicts the effect of reassignment in that classroom.
The idea is that some teachers may use reassignment as a tool to address varying student ability---for instance, they may reassign lagging students to help them keep up with their classmates.
Classrooms with higher variance in pretest scores afford more opportunities for teachers to use this reassignment strategy. 
If the strategy is widely used, and either particularly effective or ineffective at boosting students' posttest scores, there will be a correlation between classroom-level variance in pretest scores and the effect of reassignment. 

Our second hypothesis was that the proportion of students in a classroom who have been reassigned may predict classroom-level effects.
The idea here is that in classrooms with low a low proportion of students reassigned, teachers use reassignment in a more targeted fashion, so it may be more beneficial. 

Figure \ref{fig:hetRegs} plots random effects $\hat{\gamma}_j$ from model \eqref{eq:heteroReg} as a function of classroom level pretest variance and the proportion of students reassigned, respectively, with simple OLS fits. 
A positive relationship between pretest variance and $\hat{\gamma}$, and a negative relationship between proportion reassigned and $\hat{\gamma}$ are apparent, but with wide standard errors.
To test these hypotheses more formally, we re-fit model \eqref{eq:heteroReg}, adding fixed effects for the variance in pretest scores and proportion reassigned, as main effects and interacted with $Z_i$.
The model reduced the unexplained variance in classroom-level effects from \Sexpr{round(sdEff,2)} to \Sexpr{round(sdEff2,2)}---these variables explained about \Sexpr{round((1-sdEff2^2/sdEff^2)*100)}\% of the unexplained variance in treatment effects.
The coefficient on the interaction beween pretest variance and reassignment---measuring the extent to which pretest variance explains treatment effects---was estimated as \Sexpr{round(classHet['everCPTRUE:pretestVar','Estimate'],2)}, with a 95\% confidence interval of [\Sexpr{paste(round(ciHet['everCPTRUE:pretestVar',],2),collapse=',')}], so the data are compatible with large associations in either direction between pretest variance and treatment effects.
No firm conclusions may be drawn.
The coefficient on the interaction beween proportion reassigned and reassignment---measuring the extent to which classroom proportion reassigned explains treatment effects---was estimated as \Sexpr{round(classHet['everCPTRUE:meanCP','Estimate'],2)}, with a 95\% confidence interval of [\Sexpr{paste(round(ciHet['everCPTRUE:meanCP',],2),collapse=',')}], and a p-value of $p=\Sexpr{round(classHet['everCPTRUE:meanCP','Pr(>|t|)'],2)}$.
This suggests that the effect of reassignment may be lower---more negative---in classrooms in which a higher proportion of students were reassigned.
This aligns with our second hypothesis.

These effect heterogeneity analyses assume \eqref{eq:ignore1}, no unmeasured confounding. 
Unfortunately, we are not aware of methods for sensitivity analysis of the type presented in Section \ref{sec:sensitivity}, applied towards estimates of effect heterogeneity. 
In particular, unobserved confounding may vary by classroom; for instance, the structure of the propensity score match may vary with the proportion of students ever reassigned, since within-classroom matches will be scarce when this proportion is high.
For those reasons, the conclusions in this section should be taken as suggestive and exploratory. 

\section{Discussion}\label{sec:discussion}
A deeper understanding of the use of reassignment and its effects can
yield practical and theoretical dividends.
Teachers would benefit from clear guidelines as to when and whether
reassigning students to a new section may benefit that student's
learning.
A better understanding of if and when reassignment helps or hurts
student learning can contribute to our understanding of the importance
of sequence and mastery learning in intelligent tutoring systems.

Here, we estimate that, on average, reassignment hurts student
learning, perhaps as much as CTA1 helps.
That conclusion comes with two important caveats: first, although it
appears unlikely that the entire reassignment effect we estimated is
due to confounding from unmeasured variables, a large portion of the
effect might be.
That is, the magnitude of the reassignment effect we estimated may be
an artifact of unmeasured confounding---reassignment may not be as bad
as we estimate, or it may be worse.
(Of course, we cannot rule out that the entire effect is due to
confounding, or that the direction of our estimated effect is wrong.)

Secondly, there is evidence that the effect of reassignment varies
widely between classes.
Even if it hurts on average, used properly it may help.

More broadly, these issues illustrate the opportunities and perils of
analyses of log data from randomized trials of educational
technology.
Even when the randomization itself does not contribute to an analysis,
the combination of log data collected under natural conditions and a long period
of time and a posttest measuring student ability at the end of the
study can be used to gain insights on tutor use and effects.
On the other hand, log data, even from a randomized trial, is
observational, and therefore messy and subject to confounding and
other threats.
Causal modeling of log data from randomized experiments is crucial,
but difficult.

<<include=FALSE,cache=FALSE>>=
detach(dataInfo)
@

\section{Acknowledgments}
This work was partially funded by NSF grant \#DRL-1420374.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{ct}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
\end{document}
